{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 外部ライブラリをインポート\n",
    "import boto3\n",
    "import os\n",
    "import tiktoken\n",
    "import openai\n",
    "import pandas as pd\n",
    "import time\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain_cohere import CohereRerank\n",
    "from langchain import hub\n",
    "from dotenv import load_dotenv\n",
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_aws import ChatBedrock\n",
    "from langchain_community.retrievers.bedrock import AmazonKnowledgeBasesRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 環境変数読み込み\n",
    "load_dotenv(verbose=True)\n",
    "dotenv_path = os.path.join(\"./.env\")\n",
    "load_dotenv(dotenv_path)\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "cohere_api_key = os.getenv(\"COHERE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データフォルダのパス\n",
    "novel_file_path = \"../data/validation-test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI LLMの設定 (RAGの生成部分)\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-4o\",\n",
    "    openai_api_key=openai.api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bedrock Agent API呼び出し用のクライアントを作成する\n",
    "agent = boto3.client(service_name='bedrock-agent-runtime', region_name='us-east-1')\n",
    "\n",
    "# Bedrock Agent APIに渡す必要のある変数を事前に定義する\n",
    "model_id = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "model_arn = f'arn:aws:bedrock:us-east-1::foundation-model/{model_id}'\n",
    "kb_id = \"\" #ここにKnowkedge BaseのIDを入れる\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 検索手段を指定\n",
    "bedrock_retriever = AmazonKnowledgeBasesRetriever(\n",
    "  knowledge_base_id=kb_id, #ナレッジベースIDを指定\n",
    "  region_name=\"us-east-1\",\n",
    "  # クエリ分割\n",
    "  orchestrationConfiguration={\n",
    "    'queryTransformationConfiguration': {\n",
    "        'type': 'QUERY_DECOMPOSITION'\n",
    "    }\n",
    "  },\n",
    "  # 検索時の設定\n",
    "  retrieval_config={\n",
    "    \"vectorSearchConfiguration\": {\n",
    "      \"numberOfResults\": 10,\n",
    "      \"overrideSearchType\": \"HYBRID\",\n",
    "      \n",
    "      }\n",
    "    }\n",
    "  ) #ここで指定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "あなたは親切で知識豊富なチャットアシスタントです。\n",
    "<excerpts>タグには、ユーザーが知りたい情報に関連する複数のドキュメントの抜粋が含まれています。\n",
    "\n",
    "<excerpts>{context}</excerpts>\n",
    "\n",
    "これらの情報をもとに、<question>タグ内のユーザーの質問に対する回答を提供してください。\n",
    "\n",
    "<question>{question}</question>\n",
    "\n",
    "また、質問への回答は以下の点に留意してください:\n",
    "\n",
    "- <excerpts>タグの内容を参考にするが、回答に<excerpts>タグを含めないこと。\n",
    "- 簡潔に3つ以内のセンテンスで回答すること。\n",
    "- 日本語で回答すること。\n",
    "- 数量で答える問題の回答には単位を付けること.\n",
    "- 質問に対して<excerpts>タグ内にある情報で、質問に答えるための情報がない場合は「分かりません」と答えること.\n",
    "- 質問自体に誤りがあると判断される場合は「質問誤り」のみ答えること.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# プロンプトのテンプレートを定義\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# LLMを指定\n",
    "model = ChatBedrock(\n",
    "    model_id=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n",
    "    model_kwargs={\"max_tokens\": 2048},\n",
    "    region_name=\"us-east-1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# リランカー\n",
    "cohere_reranker = CohereRerank(\n",
    "    top_n=3,# Rerankで3個取得\n",
    "    cohere_api_key=cohere_api_key,\n",
    "    model=\"rerank-multilingual-v3.0\"\n",
    ")\n",
    "\n",
    "# ContextualCompressionRetrieverの準備\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=cohere_reranker,\n",
    "    base_retriever=bedrock_retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# チェーンを定義（検索 → プロンプト作成 → LLM呼び出し → 結果を取得）\n",
    "\n",
    "rag_chain_from_docs = (\n",
    "    RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain_with_source = RunnableParallel(\n",
    "    {\"context\": compression_retriever, \"question\": RunnablePassthrough()}\n",
    ").assign(answer=rag_chain_from_docs)\n",
    "\n",
    "#chain = ({\"context\": compression_retriever, \"question\": RunnablePassthrough()} | prompt | model | StrOutputParser())\n",
    "\n",
    "query = \"\" #ここにAIへの質問を入れる\n",
    "\n",
    "#result = rag_chain_with_source.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提供されたCSVファイルを読み込み\n",
    "query_df = pd.read_csv(\"../data/query.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各問題に対して回答と証拠を取得する関数\n",
    "def get_answer_and_evidence(problem):\n",
    "    # RAGによる検索・回答生成\n",
    "    result = rag_chain_with_source.invoke(problem)\n",
    "    answer = result[\"answer\"]  # 回答部分\n",
    "    evidence = result[\"context\"][0].page_content # 証拠部分を抽出\n",
    "    return answer, evidence\n",
    "\n",
    "# 各行の問題に対して処理を実行し、回答と証拠を取得\n",
    "answers = []\n",
    "evidences = []\n",
    "\n",
    "for _, row in query_df.iterrows():\n",
    "    # 問題文を取得\n",
    "    problem = row['problem']\n",
    "    print(f\"start {problem}\")\n",
    "    # 各問題に対する回答と証拠を取得\n",
    "    answer, evidence = get_answer_and_evidence(problem)\n",
    "    answers.append(answer)\n",
    "    evidences.append(evidence)\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in query_df.iterrows():\n",
    "    # 問題文を取得\n",
    "    problem = row['problem']\n",
    "    if int(_) < 53:\n",
    "        continue\n",
    "    print(f\"start {_}: {problem}\")\n",
    "    # 各問題に対する回答と証拠を取得\n",
    "    answer, evidence = get_answer_and_evidence(problem)\n",
    "    answers.append(answer)\n",
    "    evidences.append(evidence)\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrameに回答と証拠を追加\n",
    "query_df['answer'] = answers\n",
    "query_df['full_evidence'] = evidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結果をCSVファイルとして保存\n",
    "query_df.to_csv(\"../result/validation/output_with_answers_and_evidence_202401001_bedrock_full.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 回答内容を整形し、冗長な文章を削除する\n",
    "def create_reanswer(question, answer):\n",
    "    reanswer_template = \"\"\"\n",
    "    あなたはプロの編集者です。\n",
    "    以下に質問文に対する回答文があります。\n",
    "    質問文に対して回答文の中から最も簡潔に重要な内容のみを抽出してください。\n",
    "    単語のみを回答しても構いません。\n",
    "\n",
    "    # 質問文\n",
    "    {question}\t\n",
    "\n",
    "    # 回答文\n",
    "    {answer}\n",
    "    \"\"\"\n",
    "\n",
    "    custom_reanswer_prompt = PromptTemplate.from_template(reanswer_template)\n",
    "\n",
    "    reanswer_chain = custom_reanswer_prompt | llm | StrOutputParser()\n",
    "\n",
    "    return reanswer_chain.invoke({\"question\": question, \"answer\": answer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "reanswers = []\n",
    "\n",
    "for _, row in query_df.iterrows():\n",
    "    # 問題文と回答を取得\n",
    "    problem = row['problem']\n",
    "    answer = row['answer']\n",
    "    # 各問題に対する回答と証拠を取得\n",
    "    reanswer = create_reanswer(problem, answer)\n",
    "    reanswers.append(reanswer)\n",
    "\n",
    "# DataFrameに整形済み回答を追加\n",
    "query_df['answer'] = reanswers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結果をCSVファイルとして保存\n",
    "query_df.to_csv(\"../result/validation/output_with_answers_and_evidence_202401001_bedrock.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLMを使ってanswerに基づき、evidenceから200文字程度を抜き出す関数\n",
    "def extract_relevant_evidence(answer, full_evidence):\n",
    "    extract_prompt = PromptTemplate(\n",
    "        input_variables=[\"answer\", \"full_evidence\"],\n",
    "        template=\n",
    "            \"\"\"\n",
    "                f\"以下は回答と関連する証拠文です。\"\n",
    "                f\"回答に必要な部分を200文字以内で抜き出してください。\\n\"\n",
    "                f\"回答: {answer}\\n\\n\"\n",
    "                f\"証拠文: {full_evidence}\\n\"\n",
    "            \"\"\"\n",
    "    )\n",
    "    chain = extract_prompt | llm\n",
    "\n",
    "    response = chain.invoke(\n",
    "        {\"answer\": answer, \"full_evidence\": full_evidence}\n",
    "    )\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ステップ2: full_evidenceを使って関連する部分を抜き出す\n",
    "query_df['evidence'] = query_df.apply(\n",
    "    lambda row: extract_relevant_evidence(row['answer'], row['full_evidence']),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 欠損値の確認\n",
    "query_df['evidence'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_dict = {\n",
    "        \"\\n\": \"\",\n",
    "        \"\\r\": \"\",\n",
    "    }\n",
    "query_df = query_df.replace(\n",
    "        {\"answer\": replace_dict},\n",
    "        regex=True\n",
    "    )\n",
    "query_df = query_df.replace(\n",
    "        {\"evidence\": replace_dict},\n",
    "        regex=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLMを使って要約を行う関数\n",
    "def summarize_answer(answer: str) -> str:\n",
    "    # OpenAI API などの LLM を使用して要約を実行\n",
    "    \n",
    "    summarize_prompt = PromptTemplate(\n",
    "        input_variables=[\"answer\"],\n",
    "        template=\n",
    "            \"\"\"\n",
    "                以下の文章を50文字程度で要約してください。\\n\n",
    "                f\"回答: {answer}\"\n",
    "            \"\"\"\n",
    "    )\n",
    "    chain = summarize_prompt | llm\n",
    "\n",
    "    response = chain.invoke(\n",
    "        {\"answer\": answer}\n",
    "    )\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiktokenとgpt-4のトークナイザーを取得\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4-2024-08-06\")\n",
    "\n",
    "# query_df の \"answer\" 列のトークン数を計算し、50トークンを超える場合は要約を行う関数\n",
    "def check_and_summarize_answers(query_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    def summarize_if_needed(answer: str) -> str:\n",
    "        # トークン数を計算\n",
    "        token_count = len(enc.encode(answer))\n",
    "        \n",
    "        # トークン数が50を超えた場合は要約する\n",
    "        if token_count > 50:\n",
    "            # LLMを使って要約\n",
    "            summarized_answer = summarize_answer(answer)\n",
    "            return summarized_answer\n",
    "        return answer\n",
    "\n",
    "    # \"answer\" 列に対して処理を適用\n",
    "    query_df[\"answer\"] = query_df[\"answer\"].apply(summarize_if_needed)\n",
    "    return query_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_df = check_and_summarize_answers(query_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要な列（id, answer, evidence）をヘッダなしでCSVに書き出し\n",
    "query_df[['index', 'answer', 'evidence']].to_csv(\n",
    "    \"../data/evaluation/submit/predictions.csv\",\n",
    "    index=False,\n",
    "    header=False,\n",
    "    encoding=\"utf-8-sig\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# バックアップ用に別のファイルにも保存\n",
    "query_df[['index', 'answer', 'evidence']].to_csv(\n",
    "    \"../data/evaluation/submit_example/validation/predictions_20241001_prompt_remove_stopworkd.csv\",\n",
    "    index=False,\n",
    "    header=False,\n",
    "    encoding=\"utf-8-sig\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "signate-rag-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
